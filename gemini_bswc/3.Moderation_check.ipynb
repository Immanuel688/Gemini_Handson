{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a245a6e-6d14-4c81-99f9-f1306f6b30c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "client= genai.Client(api_key=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5a03686-8c38-4c4f-8df2-67bb4616ce2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion_from_messages(messages,\n",
    "                                 model=\"gemini-2.5-flash-lite\",\n",
    "                                 temperature=0,\n",
    "                                 max_tokens=500):\n",
    "    # here in google gemini you have to give the input parameters as keyword arguments and not as positional arguments\n",
    "    response = client.models.generate_content(model=model,\n",
    "                                              contents=messages,\n",
    "                                              config=types.GenerateContentConfig(\n",
    "                                                                                temperature=temperature,\n",
    "                                                                                max_output_tokens=max_tokens) \n",
    "                                             )# Pass as plain text\n",
    "    return response.text  # Extract response text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52da4194-9e37-4032-bda7-b8b6f5cd3e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flagged\n"
     ]
    }
   ],
   "source": [
    "# Call the moderation-like function\n",
    "def moderation_check(input_text):\n",
    "    messages = [\n",
    "        \"You are a content moderation AI. Reply with 'Safe' if the input is appropriate, otherwise reply with 'Flagged'.\",\n",
    "        input_text\n",
    "    ]\n",
    "    return get_completion_from_messages(messages, max_tokens=10)  # Call Gemini model\n",
    "\n",
    "# Example moderation check\n",
    "response = moderation_check(\"\"\"\n",
    "Here's the plan. We get the warhead,\n",
    "and we hold the world ransom...\n",
    "...FOR ONE MILLION DOLLARS!\n",
    "\"\"\")\n",
    "\n",
    "print(response)  # Expected output: \"Flagged\" or \"Safe\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c232b67-0e12-45a7-b56e-81a921325a65",
   "metadata": {},
   "source": [
    "# Using system instruction seperately instead of using it with messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bffdc3f7-a9ea-40c9-ac81-ecdd7c4f4cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion_from_messages(messages,system_message,\n",
    "                                 model=\"gemini-2.5-flash-lite\",\n",
    "                                 temperature=0,\n",
    "                                 max_tokens=500):\n",
    "    # here in google gemini you have to give the input parameters as keyword arguments and not as positional arguments\n",
    "    response = client.models.generate_content(model=model,\n",
    "                                              contents=messages,\n",
    "                                              config=types.GenerateContentConfig(system_instruction=system_message,\n",
    "                                                                                temperature=temperature,\n",
    "                                                                                max_output_tokens=max_tokens) \n",
    "                                             )# Pass as plain text\n",
    "    return response.text  # Extract response text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6938270c-40de-4842-9818-50d01d17578b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safe\n"
     ]
    }
   ],
   "source": [
    "# Call the moderation-like function\n",
    "def moderation_check(input_text):\n",
    "    system_message=\"You are a content moderation AI. Reply with 'Safe' if the input is appropriate, otherwise reply with 'Flagged'.\"\n",
    "    messages = input_text\n",
    "    return get_completion_from_messages(messages,system_message, max_tokens=10)  # Call Gemini model\n",
    "    # Since I've skipped temperature I need to use the keyword of that argument and pass it as keyword=value\n",
    "    # If they are entered as per position maintained in the function statement then no need of keywords\n",
    "\n",
    "# Example moderation check\n",
    "response = moderation_check(\"\"\"\n",
    "Here's the plan. We get the warhead,\n",
    "and we hold the world ransom...\n",
    "...FOR ONE MILLION DOLLARS!\n",
    "\"\"\")\n",
    "\n",
    "print(response)  # Expected output: \"Flagged\" or \"Safe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1347bd4a-c17e-4bd4-9a6a-8915ec6a3328",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
